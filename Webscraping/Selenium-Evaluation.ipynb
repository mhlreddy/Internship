{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc9bb4c7",
   "metadata": {},
   "source": [
    "# WEB SCRAPING – ASSIGNMENT 2\n",
    "\n",
    "### Web Scraping using Selenium\n",
    "#### Intern Name: Lakshminarayanareddy Marapareddygarihanumanthu\n",
    "#### Internship Number: DS2405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5809dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /home/lreddy/anaconda3/lib/python3.9/site-packages (4.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /home/lreddy/anaconda3/lib/python3.9/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /home/lreddy/anaconda3/lib/python3.9/site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: websocket-client>=1.8.0 in /home/lreddy/anaconda3/lib/python3.9/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /home/lreddy/anaconda3/lib/python3.9/site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /home/lreddy/anaconda3/lib/python3.9/site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: trio~=0.17 in /home/lreddy/anaconda3/lib/python3.9/site-packages (from selenium) (0.25.1)\n",
      "Requirement already satisfied: outcome in /home/lreddy/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sortedcontainers in /home/lreddy/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/lreddy/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.2.1)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /home/lreddy/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: idna in /home/lreddy/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/lreddy/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/lreddy/anaconda3/lib/python3.9/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /home/lreddy/anaconda3/lib/python3.9/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/lreddy/anaconda3/lib/python3.9/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "# Install and import required modules \n",
    "!pip install selenium\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver  # Allows you to launch/initialise a browser\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By   # Allows you to search for things using specific parameters like ID, Class name,...\n",
    "import time   # Allows to stop search engine for user specified time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb073c2",
   "metadata": {},
   "source": [
    "Q1: In this question you have to scrape data using the filters available on the webpage You have to use the location and salary filter.\n",
    "\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "\n",
    "The task will be done as shown in the below steps:\n",
    "    \n",
    "1. first get the web page https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "259af231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Chrome driver\n",
    "driver1 = webdriver.Chrome()\n",
    "\n",
    "# Opening the required web page https://www.naukri.com/ on automated Chrome browser\n",
    "driver1.get('https://www.naukri.com/')\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f63ae9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "role = driver1.find_element(By.XPATH,'/html/body/div[1]/div[7]/div/div/div[1]/div/div/div/div[1]/div/input')\n",
    "role.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d82f7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click the search button.\n",
    "search = driver1.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a92a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Applying the location filter and salary filter by checking the respective boxes\n",
    "location_filter = driver1.find_element(By.XPATH,'/html/body/div/div/main/div[1]/div[1]/div/div/div[2]/div[4]/div[2]/div[3]/label/i')\n",
    "location_filter.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "salary_filter = driver1.find_element(By.XPATH,'/html/body/div/div/main/div[1]/div[1]/div/div/div[2]/div[5]/div[2]/div[2]/label/i')\n",
    "salary_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "808d0a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty lists to store scraped data of job_title, job_location, company_name and experience_required\n",
    "\n",
    "job_title = []\n",
    "job_location = []\n",
    "company_name = []\n",
    "experience_required = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdfbf027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping the data for the first 10 jobs results.\n",
    "\n",
    "# Scraping job title from the given web page\n",
    "title_tags = driver1.find_elements(By.XPATH,'//div[@class=\"cust-job-tuple layout-wrapper lay-2 sjw__tuple \"]/div/a')\n",
    "for i in title_tags[:10]:\n",
    "    title = i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "# Scraping job location from the given web page\n",
    "location_tags = driver1.find_elements(By.XPATH,'//span[@class=\"locWdth\"]')\n",
    "for i in location_tags[:10]:\n",
    "    location = i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "# Scraping company names from the given web page\n",
    "company_tags = driver1.find_elements(By.XPATH,'//div[@class=\" row2\"]/span/a[1]')\n",
    "for i in company_tags[:10]:\n",
    "    company = i.text\n",
    "    company_name.append(company)\n",
    "    \n",
    "# Scraping job experience required from the given web page\n",
    "experience_tags = driver1.find_elements(By.XPATH,'//span[@class=\"expwdth\"]')\n",
    "for i in experience_tags[:10]:\n",
    "    exp = i.text\n",
    "    experience_required.append(exp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "831c0328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title), len(job_location), len(company_name), len(experience_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8dadcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company_name</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Delhi / NCR, Pune, Bengaluru</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist III</td>\n",
       "      <td>Gurugram</td>\n",
       "      <td>Flutter</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>Sociomix</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data scientist</td>\n",
       "      <td>Gurugram</td>\n",
       "      <td>Growthjockey</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...</td>\n",
       "      <td>Essenware</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist (Telco)</td>\n",
       "      <td>Gurugram, Bengaluru</td>\n",
       "      <td>PayU</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurugram</td>\n",
       "      <td>Orange Business Services</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Ericsson</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Times Internet</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Ericsson</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Title                                           Location  \\\n",
       "0          Data Scientist                       Delhi / NCR, Pune, Bengaluru   \n",
       "1      Data Scientist III                                           Gurugram   \n",
       "2          Data Scientist                                          New Delhi   \n",
       "3          Data scientist                                           Gurugram   \n",
       "4          Data Scientist  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...   \n",
       "5  Data Scientist (Telco)                                Gurugram, Bengaluru   \n",
       "6          Data Scientist                                           Gurugram   \n",
       "7          Data Scientist                                              Noida   \n",
       "8          Data Scientist                                              Noida   \n",
       "9          Data Scientist                                              Noida   \n",
       "\n",
       "               Company_name Experience  \n",
       "0                     Wipro   6-11 Yrs  \n",
       "1                   Flutter    2-6 Yrs  \n",
       "2                  Sociomix    0-5 Yrs  \n",
       "3              Growthjockey    0-1 Yrs  \n",
       "4                 Essenware    2-5 Yrs  \n",
       "5                      PayU    2-7 Yrs  \n",
       "6  Orange Business Services    3-5 Yrs  \n",
       "7                  Ericsson    3-7 Yrs  \n",
       "8            Times Internet    3-8 Yrs  \n",
       "9                  Ericsson    3-7 Yrs  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dataframe of the scraped data\n",
    "df_naukri = pd.DataFrame({'Title': job_title, 'Location': job_location, 'Company_name': company_name, 'Experience': experience_required})\n",
    "df_naukri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efc07a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Close the browser session \n",
    "driver1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d384037a",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "\n",
    "This task will be done in following steps:\n",
    "\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f1ea262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Chrome driver\n",
    "driver2 = webdriver.Chrome()\n",
    "\n",
    "# Opening the required web page https://www.shine.com/ on automated Chrome browser\n",
    "driver2.get('https://www.shine.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0410166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter “Data Scientist” in “Job title, Skills” field \n",
    "jobrole = driver2.find_element(By.XPATH,'/html/body/div/header/div[3]/div/div/div[1]/div/input')\n",
    "jobrole.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c071238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click the search button.\n",
    "search_button = driver2.find_element(By.XPATH,'/html/body/div/header/div[3]/div/div/div[1]/div/span/i')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "735a4a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter “Data Scientist” in “Job title, Skills” field  and “Bangalore” in “enter the location” field\n",
    "role = driver2.find_element(By.XPATH,'/html/body/div/div[4]/div/div[2]/div[2]/div/form/div/div[1]/ul/li[1]/div/input')\n",
    "role.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a934664",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = driver2.find_element(By.XPATH,'/html/body/div/div[4]/div/div[2]/div[2]/div/form/div/div[1]/ul/li[2]/div/input')\n",
    "location.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "724096d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click the search jobs button.\n",
    "search_jobs = driver2.find_element(By.XPATH,'/html/body/div/div[4]/div/div[2]/div[2]/div/form/div/div[2]/div/button')\n",
    "search_jobs.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42c88115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty lists to store scraped data of job_title, job_location, company_name and experience_required\n",
    "\n",
    "job_title = []\n",
    "job_location = []\n",
    "company_name = []\n",
    "experience_required = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3752b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping the data for the first 10 jobs results.\n",
    "\n",
    "# Scraping job title from the given web page\n",
    "title_tags = driver2.find_elements(By.XPATH,'//strong[@class=\"jobCard_pReplaceH2__xWmHg\"]/p/a')\n",
    "for i in title_tags[:10]:\n",
    "    title = i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "# Scraping job location from the given web page\n",
    "location_tags = driver2.find_elements(By.XPATH,'//div[@class=\"jobCard_jobCard_lists_item__YxRkV jobCard_locationIcon__zrWt2\"]')\n",
    "for i in location_tags[:10]:\n",
    "    location = i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "# Scraping company names from the given web page\n",
    "company_tags = driver2.find_elements(By.XPATH,'//div[@class=\"jobCard_jobCard_cName__mYnow\"]/span')\n",
    "for i in company_tags[:10]:\n",
    "    company = i.text\n",
    "    company_name.append(company)\n",
    "    \n",
    "# Scraping job experience required from the given web page\n",
    "experience_tags = driver2.find_elements(By.XPATH,'//div[@class=\" jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t\"]')\n",
    "for i in experience_tags[:10]:\n",
    "    exp = i.text\n",
    "    experience_required.append(exp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07a78d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist Recruitment',\n",
       " 'Data Scientist',\n",
       " 'Hiring For Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Data Analyst',\n",
       " 'Senior Associate Data Engineering Level 2',\n",
       " 'Hr Data Analyst']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d78db8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bangalore',\n",
       " 'Bangalore\\n+1',\n",
       " 'Bangalore',\n",
       " 'Bangalore\\n+13',\n",
       " 'Bangalore\\n+10',\n",
       " 'Bangalore\\n+15',\n",
       " 'Bangalore\\n+2',\n",
       " 'Bangalore\\n+4',\n",
       " 'Bangalore\\n+7',\n",
       " 'Bangalore']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c3e7384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comosrich it & consulting services',\n",
       " 'dotcod innovation private limited',\n",
       " 'meraki it solutions',\n",
       " 'kavya interprises',\n",
       " 'vaishnavi services',\n",
       " 'neelima staffing solution',\n",
       " 'talent toppers llp',\n",
       " 'aryan technology',\n",
       " 'tlg india private limited',\n",
       " 'leverage business solutions private...']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7db2e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3 to 8 Yrs',\n",
       " '2 to 6 Yrs',\n",
       " '3 to 6 Yrs',\n",
       " '0 to 4 Yrs',\n",
       " '18 to >25 Yrs',\n",
       " '0 to 4 Yrs',\n",
       " '0 to 3 Yrs',\n",
       " '0 to 4 Yrs',\n",
       " '5 to 9 Yrs',\n",
       " '4 to 8 Yrs']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience_required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bdbf07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company_name</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>comosrich it &amp; consulting services</td>\n",
       "      <td>3 to 8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore\\n+1</td>\n",
       "      <td>dotcod innovation private limited</td>\n",
       "      <td>2 to 6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>meraki it solutions</td>\n",
       "      <td>3 to 6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist Recruitment</td>\n",
       "      <td>Bangalore\\n+13</td>\n",
       "      <td>kavya interprises</td>\n",
       "      <td>0 to 4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore\\n+10</td>\n",
       "      <td>vaishnavi services</td>\n",
       "      <td>18 to &gt;25 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hiring For Data Scientist</td>\n",
       "      <td>Bangalore\\n+15</td>\n",
       "      <td>neelima staffing solution</td>\n",
       "      <td>0 to 4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore\\n+2</td>\n",
       "      <td>talent toppers llp</td>\n",
       "      <td>0 to 3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore\\n+4</td>\n",
       "      <td>aryan technology</td>\n",
       "      <td>0 to 4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Associate Data Engineering Level 2</td>\n",
       "      <td>Bangalore\\n+7</td>\n",
       "      <td>tlg india private limited</td>\n",
       "      <td>5 to 9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hr Data Analyst</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>leverage business solutions private...</td>\n",
       "      <td>4 to 8 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Title        Location  \\\n",
       "0                             Data Scientist       Bangalore   \n",
       "1                             Data Scientist   Bangalore\\n+1   \n",
       "2                             Data Scientist       Bangalore   \n",
       "3                 Data Scientist Recruitment  Bangalore\\n+13   \n",
       "4                             Data Scientist  Bangalore\\n+10   \n",
       "5                  Hiring For Data Scientist  Bangalore\\n+15   \n",
       "6                             Data Scientist   Bangalore\\n+2   \n",
       "7                               Data Analyst   Bangalore\\n+4   \n",
       "8  Senior Associate Data Engineering Level 2   Bangalore\\n+7   \n",
       "9                            Hr Data Analyst       Bangalore   \n",
       "\n",
       "                             Company_name     Experience  \n",
       "0      comosrich it & consulting services     3 to 8 Yrs  \n",
       "1       dotcod innovation private limited     2 to 6 Yrs  \n",
       "2                     meraki it solutions     3 to 6 Yrs  \n",
       "3                       kavya interprises     0 to 4 Yrs  \n",
       "4                      vaishnavi services  18 to >25 Yrs  \n",
       "5               neelima staffing solution     0 to 4 Yrs  \n",
       "6                      talent toppers llp     0 to 3 Yrs  \n",
       "7                        aryan technology     0 to 4 Yrs  \n",
       "8               tlg india private limited     5 to 9 Yrs  \n",
       "9  leverage business solutions private...     4 to 8 Yrs  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dataframe of the scraped data\n",
    "\n",
    "df_shine = pd.DataFrame({'Title': job_title, 'Location': job_location, 'Company_name': company_name, 'Experience': experience_required})\n",
    "df_shine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c76b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Close the browser session \n",
    "driver2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf82f44",
   "metadata": {},
   "source": [
    "Q3: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/product-\n",
    "reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=F\n",
    "LIPKART\n",
    "\n",
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "    \n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2335219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Chrome driver\n",
    "driver3 = webdriver.Chrome()\n",
    "\n",
    "# Opening the required web page https://www.naukri.com/ on automated Chrome browser\n",
    "driver3.get('https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART')    \n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b4c03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty lists to store scraped data of rating, review_summary, and full_review\n",
    "\n",
    "ratings = []\n",
    "reviews_summary = []\n",
    "full_reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cd06d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping rating from review\n",
    "rating_tags = driver3.find_elements(By.XPATH,'//div[@class=\"XQDdHH Ga3i8K\"]')\n",
    "for i in rating_tags:\n",
    "    star = i.text\n",
    "    ratings.append(star) \n",
    "\n",
    "# Scraping summary from review\n",
    "summary_tags = driver3.find_elements(By.XPATH,'//p[@class=\"z9E0IG\"]')\n",
    "for i in summary_tags:\n",
    "    summa = i.text\n",
    "    reviews_summary.append(summa)\n",
    "    \n",
    "# Scraping full review from review\n",
    "summary_tags = driver3.find_elements(By.XPATH,'//div[@class=\"ZmyHeo\"]')\n",
    "for i in summary_tags:\n",
    "    full = i.text\n",
    "    full_reviews.append(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f03cf2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Summary</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Feeling awesome after getting the delivery of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Good Camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>This is amazing at all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific</td>\n",
       "      <td>Very very good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>Perfect Product!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Must buy!</td>\n",
       "      <td>It’s really awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Photos super</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>Classy product</td>\n",
       "      <td>Camera is awesome\\nBest battery backup\\nA perf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>It’s very good battery life and display and vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific purchase</td>\n",
       "      <td>Value for money 😍</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rating       Review Summary  \\\n",
       "0      5    Worth every penny   \n",
       "1      5  Best in the market!   \n",
       "2      5            Wonderful   \n",
       "3      5             Terrific   \n",
       "4      5            Just wow!   \n",
       "5      5            Must buy!   \n",
       "6      5     Perfect product!   \n",
       "7      5       Classy product   \n",
       "8      5            Fabulous!   \n",
       "9      5    Terrific purchase   \n",
       "\n",
       "                                         Full Review  \n",
       "0  Feeling awesome after getting the delivery of ...  \n",
       "1                                        Good Camera  \n",
       "2                             This is amazing at all  \n",
       "3                                     Very very good  \n",
       "4                                  Perfect Product!!  \n",
       "5                                It’s really awesome  \n",
       "6                                       Photos super  \n",
       "7  Camera is awesome\\nBest battery backup\\nA perf...  \n",
       "8  It’s very good battery life and display and vi...  \n",
       "9                                  Value for money 😍  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews10 = pd.DataFrame({'Rating': ratings, 'Review Summary': reviews_summary, 'Full Review': full_reviews})\n",
    "df_reviews10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca403861",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[10]\"}\n  (Session info: chrome=116.0.5845.140); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n#0 0x5c53f3415e23 <unknown>\n#1 0x5c53f313e7a7 <unknown>\n#2 0x5c53f317d1d3 <unknown>\n#3 0x5c53f317d2c1 <unknown>\n#4 0x5c53f31b8a04 <unknown>\n#5 0x5c53f319e03d <unknown>\n#6 0x5c53f31b6369 <unknown>\n#7 0x5c53f319dde3 <unknown>\n#8 0x5c53f3171a7b <unknown>\n#9 0x5c53f317281e <unknown>\n#10 0x5c53f33d7638 <unknown>\n#11 0x5c53f33db507 <unknown>\n#12 0x5c53f33e5c4c <unknown>\n#13 0x5c53f33dc136 <unknown>\n#14 0x5c53f33aa9cf <unknown>\n#15 0x5c53f33ffb98 <unknown>\n#16 0x5c53f33ffd68 <unknown>\n#17 0x5c53f340ecb3 <unknown>\n#18 0x74a305e94ac3 <unknown>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_236843/2034125134.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# coping full XPath of next button from web page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mnext_button\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[10]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0mnext_button\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'[name=\"{value}\"]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFIND_ELEMENT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"using\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"value\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWebElement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"alert\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[10]\"}\n  (Session info: chrome=116.0.5845.140); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n#0 0x5c53f3415e23 <unknown>\n#1 0x5c53f313e7a7 <unknown>\n#2 0x5c53f317d1d3 <unknown>\n#3 0x5c53f317d2c1 <unknown>\n#4 0x5c53f31b8a04 <unknown>\n#5 0x5c53f319e03d <unknown>\n#6 0x5c53f31b6369 <unknown>\n#7 0x5c53f319dde3 <unknown>\n#8 0x5c53f3171a7b <unknown>\n#9 0x5c53f317281e <unknown>\n#10 0x5c53f33d7638 <unknown>\n#11 0x5c53f33db507 <unknown>\n#12 0x5c53f33e5c4c <unknown>\n#13 0x5c53f33dc136 <unknown>\n#14 0x5c53f33aa9cf <unknown>\n#15 0x5c53f33ffb98 <unknown>\n#16 0x5c53f33ffd68 <unknown>\n#17 0x5c53f340ecb3 <unknown>\n#18 0x74a305e94ac3 <unknown>\n"
     ]
    }
   ],
   "source": [
    "# Scraping data for first 100 reviews\n",
    "\n",
    "ratings_100 = []\n",
    "reviews_summary100 = []\n",
    "full_reviews100 = []\n",
    "\n",
    "def reviews(ratings_100,reviews_summary100,full_reviews100):\n",
    "    # Scraping first 10 ratings from review\n",
    "    rating_tags = driver3.find_elements(By.XPATH,'//div[@class=\"XQDdHH Ga3i8K\"]')\n",
    "    for i in rating_tags:\n",
    "        star = i.text\n",
    "        ratings_100.append(star)\n",
    "        \n",
    "    # Scraping first 10 summaries from review\n",
    "    summary_tags = driver3.find_elements(By.XPATH,'//p[@class=\"z9E0IG\"]')\n",
    "    for i in summary_tags:\n",
    "        summa = i.text\n",
    "        reviews_summary100.append(summa)\n",
    "        \n",
    "    # Scraping first 10 full reviews from review\n",
    "    summary_tags = driver3.find_elements(By.XPATH,'//div[@class=\"ZmyHeo\"]')\n",
    "    for i in summary_tags:\n",
    "        full = i.text\n",
    "        full_reviews100.append(full)\n",
    "        \n",
    "        \n",
    "\n",
    "reviews(ratings_100,reviews_summary100,full_reviews100)\n",
    "# coping full XPath of next button from web page\n",
    "next_button = driver3.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[2]')\n",
    "next_button.click()\n",
    "time.sleep(3)\n",
    "reviews(ratings_100,reviews_summary100,full_reviews100)\n",
    "\n",
    "# coping full XPath of next button from web page\n",
    "next_button = driver3.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[3]')\n",
    "next_button.click()\n",
    "time.sleep(3)\n",
    "reviews(ratings_100,reviews_summary100,full_reviews100)\n",
    "\n",
    "# coping full XPath of next button from web page\n",
    "next_button = driver3.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[4]')\n",
    "next_button.click()\n",
    "time.sleep(3)\n",
    "reviews(ratings_100,reviews_summary100,full_reviews100)\n",
    "\n",
    "\n",
    "# coping full XPath of next button from web page\n",
    "next_button = driver3.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[5]')\n",
    "next_button.click()\n",
    "time.sleep(3)\n",
    "reviews(ratings_100,reviews_summary100,full_reviews100)\n",
    "\n",
    "\n",
    "# coping full XPath of next button from web page\n",
    "next_button = driver3.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[6]')\n",
    "next_button.click()\n",
    "time.sleep(3)\n",
    "reviews(ratings_100,reviews_summary100,full_reviews100)\n",
    "\n",
    "\n",
    "# coping full XPath of next button from web page\n",
    "next_button = driver3.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[7]')\n",
    "next_button.click()\n",
    "time.sleep(3)\n",
    "reviews(ratings_100,reviews_summary100,full_reviews100)\n",
    "\n",
    "\n",
    "# coping full XPath of next button from web page\n",
    "next_button = driver3.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[8]')\n",
    "next_button.click()\n",
    "time.sleep(3)\n",
    "reviews(ratings_100,reviews_summary100,full_reviews100)\n",
    "\n",
    "\n",
    "# coping full XPath of next button from web page\n",
    "next_button = driver3.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[9]')\n",
    "next_button.click()\n",
    "time.sleep(3)\n",
    "reviews(ratings_100,reviews_summary100,full_reviews100)\n",
    "\n",
    "\n",
    "# coping full XPath of next button from web page\n",
    "next_button = driver3.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[10]')\n",
    "next_button.click()\n",
    "time.sleep(3)\n",
    "reviews(ratings_100,reviews_summary100,full_reviews100)\n",
    "\n",
    "\n",
    "# coping full XPath of next button from web page\n",
    "next_button = driver3.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[11]')\n",
    "next_button.click()\n",
    "time.sleep(3)\n",
    "reviews(ratings_100,reviews_summary100,full_reviews100)\n",
    "\n",
    "# coping full XPath of next button from web page\n",
    "next_button = driver3.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "next_button.click()\n",
    "time.sleep(3)\n",
    "reviews(ratings_100,reviews_summary100,full_reviews100)\n",
    "\n",
    "# coping full XPath of next button from web page\n",
    "next_button = driver3.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[13]')\n",
    "next_button.click()\n",
    "time.sleep(3)\n",
    "reviews(ratings_100,reviews_summary100,full_reviews100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f85ad13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '4', '5', '5', '5', '5', '5', '5', '5', '5', '5', '4', '5', '4', '5', '5', '5', '5', '5', '5', '5', '5', '4', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '4', '5', '4']\n"
     ]
    }
   ],
   "source": [
    "print(ratings_100[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebc202cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Close the browser session \n",
    "driver3.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b41b8b1",
   "metadata": {},
   "source": [
    "Q4: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2aaefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Chrome driver\n",
    "driver4 = webdriver.Chrome()\n",
    "\n",
    "# Opening the required web page https://www.naukri.com/ on automated Chrome browser\n",
    "driver4.get('https://www.flipkart.com/')    \n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d32c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sending \"sneakers\" to search and then enter for search\n",
    "product = driver4.find_element(By.XPATH,'/html/body/div[1]/div/div[1]/div/div/div/div/div[1]/div/div[1]/div/div[1]/div[1]/header/div[1]/div[2]/form/div/div/input')\n",
    "product.send_keys('sneakers'+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "415e750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty lists to store scraped data of brand, product description and price\n",
    "\n",
    "brand = []\n",
    "product_description = []\n",
    "price = []\n",
    "\n",
    "# Scraping brand from search\n",
    "brand_tags = driver4.find_elements(By.XPATH,'//div[@class=\"syl9yP\"]')\n",
    "for i in brand_tags:\n",
    "    name = i.text\n",
    "    brand.append(name) \n",
    "\n",
    "# Scraping product description from search\n",
    "pd_tags = driver4.find_elements(By.XPATH,'/html/body/div/div/div[3]/div[1]/div[2]/div[2]/div/div[1]/div/div/a[1]')\n",
    "for i in pd_tags:\n",
    "    pds = i.text\n",
    "    product_description.append(pds)\n",
    "    \n",
    "# Scraping price from search\n",
    "price_tags = driver4.find_elements(By.XPATH,'//div[@class=\"Nx9bqj\"]')\n",
    "for i in price_tags:\n",
    "    pr = i.text\n",
    "    price.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d332c418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 1 40\n"
     ]
    }
   ],
   "source": [
    "print(len(brand), len(product_description), len(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f451eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>₹297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>₹297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deals4you</td>\n",
       "      <td>₹399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aadi</td>\n",
       "      <td>₹299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. POLO ASSN.</td>\n",
       "      <td>₹1,249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aadi</td>\n",
       "      <td>₹439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>₹297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>₹297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>X Xiota</td>\n",
       "      <td>₹699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>₹1,047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>₹499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>₹297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>₹1,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>₹1,047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ATOM</td>\n",
       "      <td>₹1,685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>₹1,043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>aadi</td>\n",
       "      <td>₹299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>₹449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>aadi</td>\n",
       "      <td>₹425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RapidBox</td>\n",
       "      <td>₹795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>₹297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>₹297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>₹1,298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>asian</td>\n",
       "      <td>₹808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>asian</td>\n",
       "      <td>₹746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>asian</td>\n",
       "      <td>₹749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Deals4you</td>\n",
       "      <td>₹399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>JUMPLITE</td>\n",
       "      <td>₹999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>CAMPUS</td>\n",
       "      <td>₹1,608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Ifah</td>\n",
       "      <td>₹699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Deals4you</td>\n",
       "      <td>₹399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>₹297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>RUN SEVEN</td>\n",
       "      <td>₹999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>₹1,680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>VENDOZ</td>\n",
       "      <td>₹599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>₹1,075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Aeonik</td>\n",
       "      <td>₹497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Zixer</td>\n",
       "      <td>₹699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>URBANBOX</td>\n",
       "      <td>₹449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ATOM</td>\n",
       "      <td>₹1,139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Brand   Price\n",
       "0            BRUTON    ₹297\n",
       "1            BRUTON    ₹297\n",
       "2         Deals4you    ₹399\n",
       "3              aadi    ₹299\n",
       "4   U.S. POLO ASSN.  ₹1,249\n",
       "5              aadi    ₹439\n",
       "6            BRUTON    ₹297\n",
       "7            BRUTON    ₹297\n",
       "8           X Xiota    ₹699\n",
       "9              PUMA  ₹1,047\n",
       "10           BRUTON    ₹499\n",
       "11           BRUTON    ₹297\n",
       "12             PUMA  ₹1,200\n",
       "13             PUMA  ₹1,047\n",
       "14             ATOM  ₹1,685\n",
       "15             PUMA  ₹1,043\n",
       "16             aadi    ₹299\n",
       "17           BRUTON    ₹449\n",
       "18             aadi    ₹425\n",
       "19         RapidBox    ₹795\n",
       "20           BRUTON    ₹297\n",
       "21           BRUTON    ₹297\n",
       "22             PUMA  ₹1,298\n",
       "23            asian    ₹808\n",
       "24            asian    ₹746\n",
       "25            asian    ₹749\n",
       "26        Deals4you    ₹399\n",
       "27         JUMPLITE    ₹999\n",
       "28           CAMPUS  ₹1,608\n",
       "29             Ifah    ₹699\n",
       "30        Deals4you    ₹399\n",
       "31           BRUTON    ₹297\n",
       "32        RUN SEVEN    ₹999\n",
       "33             PUMA  ₹1,680\n",
       "34           VENDOZ    ₹599\n",
       "35             PUMA  ₹1,075\n",
       "36           Aeonik    ₹497\n",
       "37            Zixer    ₹699\n",
       "38         URBANBOX    ₹449\n",
       "39             ATOM  ₹1,139"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating data frame of scraped data\n",
    "df_sneakers = pd.DataFrame({'Brand':  brand, 'Price': price})\n",
    "df_sneakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be24d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver4.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9e50a2",
   "metadata": {},
   "source": [
    "Q5: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then set CPU\n",
    "Type filter to “Intel Core i7” as shown in the below image:\n",
    "    \n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9aedce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Chrome driver\n",
    "driver5 = webdriver.Chrome()\n",
    "\n",
    "# Opening the required web page https://www.amazon.in/ on automated Chrome browser\n",
    "driver5.get('https://www.amazon.in/')    \n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82935b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entering Laptop in the search field\n",
    "product = driver5.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')\n",
    "product.send_keys('Laptop'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c45eb5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Applying the CPU type filter checking the respective box\n",
    "CPU_filter = driver5.find_element(By.XPATH,'/html/body/div[1]/div[1]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[7]/ul[2]/span/span[10]/li/span/a/div/label/i')\n",
    "CPU_filter.click()\n",
    "\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30f351dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty lists to store scraped data of title, ratings and price\n",
    "title = []\n",
    "rating = []\n",
    "price = []\n",
    "\n",
    "# Scraping title from search\n",
    "title_tags = driver5.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "for i in title_tags:\n",
    "    name = i.text\n",
    "    title.append(name) \n",
    "\n",
    "# Scraping rating from search\n",
    "rating_tags = driver5.find_elements(By.XPATH,'//a[@class=\"a-link-normal s-underline-text s-underline-link-text s-link-style\"]/span')\n",
    "for i in rating_tags :\n",
    "    star = i.text\n",
    "    rating.append(star)\n",
    "    \n",
    "# Scraping price from search\n",
    "price_tags = driver5.find_elements(By.XPATH,'//span[@class=\"a-price\"]/span')\n",
    "for i in price_tags:\n",
    "    pr = i.text\n",
    "    price.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8eb60480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MSI Modern 14, Intel 12th Gen. i7-1255U, 36CM FHD 60Hz Laptop (16GB/512GB NVMe SSD/Windows 11 Home/Intel Iris Xe Graphics/Classic Black/1.4Kg), C12M-459IN',\n",
       " 'HP Laptop 15s, 12th Gen Intel Core i7-1255U, 15.6-inch (39.6 cm), FHD, 16GB DDR4, 512GB SSD, Intel Iris Xe graphics, Backlit KB, Thin & light (Win 11, MSO 2021, Silver, 1.69 kg), fq5190TU',\n",
       " 'Lenovo IdeaPad 3 Intel Core i7 12th Gen 1255U - (16 GB/512 GB SSD/Windows 11 Home) 15IAU7 Laptop 82RK011GIN (39.62 cm, Arctic Grey, 1.63 Kg, with MS Office)',\n",
       " 'ASUS Vivobook 15, Intel Core i7-12650H 12th Gen, 15.6\" (39.62 cm) FHD, Thin and Light Laptop (16GB/512GB/Win11/Office 2021/Blue/1.7 kg), X1502ZA-EJ741WS',\n",
       " 'HP Pavilion 14 12th Gen Intel Core i7 16GB SDRAM/1TB SSD 14 inch(35.6cm) FHD,IPS,Micro-Edge Display/Intel Iris Xe Graphics/B&O/Win 11/Alexa Built-in/Backlit KB/FPR/MSO 2021/Natural Silver,14-dv2015TU',\n",
       " 'Lenovo IdeaPad Slim 5 Intel Core i7 13700H 16\" (40.6cm) 2.5K IPS 350Nits Laptop (16GB/1TB SSD/Win 11/Office 2021/Backlit KB/FHD 1080p +IR Camera/Alexa/3 month Game Pass/Cloud Grey/1.9Kg), 82XF0078IN',\n",
       " 'Dell [Smartchoice Inspiron 5430 Thin & Light Laptop, 13th Gen Intel Core i7-1360P/16GB/1TB SSD/14.0\" (35.56cm) FHD+ WVA 250 nits/Backlit KB + FPR/Windows 11 + MSO\\'21/15 Month McAfee/Silver/1.59kg',\n",
       " 'HP Pavilion 14, 11th Gen Intel Core i7-1195G7, 14-inch (35.6 cm), FHD, 16GB DDR4, 1TB SSD, Intel Iris Xe graphics, FPR, Backlit KB, Audio by B&O (Win 11, MSO 2019, Silver, 1.41 kg), dv1029TU',\n",
       " 'Dell Latitude 7480 14in FHD Laptop PC - Intel Core i7-6600U 2.6GHz 16GB 512GB SSD Windows 10 Professional (Renewed)',\n",
       " 'HP Pavilion 15, 13th Gen Intel Core i7-1360P, 15.6-inch (39.6 cm), FHD, 16GB DDR4, 1TB SSD, Intel Iris Xᵉ graphics, FPR, Backlit KB, Audio by B&O (Win 11, MSO 2021, Silver, 1.74 kg), eg3036TU']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7cada53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver5.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58966deb",
   "metadata": {},
   "source": [
    "Q6: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "    \n",
    "1. First get the webpagehttps://www.azquotes.com/\n",
    "2. Click on Top Quote\n",
    "3. Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b35f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9476ff40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9822c7be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe741ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b20a8798",
   "metadata": {},
   "source": [
    "Q7: Write a python program to display list of respected former Prime Ministers of India (i.e. Name, Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1\n",
    "\n",
    "scrap the mentioned data and make the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fe782e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff5c8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62adec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b31a57e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4884c87f",
   "metadata": {},
   "source": [
    "Q8: Write a python program to display list of 50 Most expensive cars in the world\n",
    "(i.e. Car name and Price) from https://www.motor1.com/\n",
    "\n",
    "This task will be done in following steps:\n",
    "    \n",
    "1. First get the webpage https://www.motor1.com/\n",
    "2. Then You have to type in the search bar ’50 most expensive cars’\n",
    "3. Then click on 50 most expensive cars in the world..\n",
    "4. Then scrap the mentioned data and make the dataframe.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1691e52b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b7a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
